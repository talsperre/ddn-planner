{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/shashanks./Downloads/Installations/ddn/\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from ddn.pytorch.node import *\n",
    "from scipy.linalg import block_diag\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bernstein import bernstein_coeff_order10_new\n",
    "from ddn.pytorch.node import AbstractDeclarativeNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fin = 8.0\n",
    "num = 20\n",
    "\n",
    "tot_time = np.linspace(0.0, t_fin, num)\n",
    "tot_time_copy = tot_time.reshape(num, 1)\n",
    "P, Pdot, Pddot = bernstein_coeff_order10_new(10, tot_time_copy[0], tot_time_copy[-1], tot_time_copy)\n",
    "nvar = np.shape(P)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_eq_mat = np.vstack((P[0], Pdot[0], Pddot[0], P[-1], Pdot[-1], Pddot[-1]))\n",
    "A_eq_np = block_diag(A_eq_mat, A_eq_mat)\n",
    "Q_np = 10 * block_diag(np.dot(Pddot.T, Pddot), np.dot(Pddot.T, Pddot))\n",
    "q_np = np.zeros(2 * nvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QPNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPNode(AbstractDeclarativeNode):\n",
    "    def __init__(self, Q_np, q_np, A_eq_np, rho=1.0, nvar=22, maxiter=1000):\n",
    "        super().__init__()\n",
    "        self.rho = rho\n",
    "        self.nvar = nvar\n",
    "        self.maxiter = maxiter\n",
    "        self.Q = torch.tensor(Q_np, dtype=torch.double).to(device)\n",
    "        self.q = torch.tensor(q_np, dtype=torch.double).to(device)\n",
    "        self.A = torch.tensor(A_eq_np, dtype=torch.double).to(device)\n",
    "    \n",
    "    def objective(self, b, lamda, y):\n",
    "        \"\"\"\n",
    "        b: (B x 12)\n",
    "        lamda: (B x 22)\n",
    "        y: (B x 22)\n",
    "        \"\"\"\n",
    "        lamda = lamda.transpose(0, 1)\n",
    "        y = y.transpose(0, 1)\n",
    "        cost_mat = self.rho * torch.matmul(self.A.T, self.A) + self.Q\n",
    "        lincost_mat = -self.rho * torch.matmul(b, self.A).T + self.q.view(-1, 1) - lamda\n",
    "        f = 0.5 * torch.diag(torch.matmul(y.T, torch.matmul(cost_mat, y))) + torch.diag(torch.matmul(lincost_mat.T, y))\n",
    "        return f\n",
    "    \n",
    "    def compute_augmented_lagrangian(self, b, lamda):\n",
    "        \"\"\"\n",
    "        b: (12,)\n",
    "        lamda: (22,)\n",
    "        \"\"\"\n",
    "        cost_mat = self.rho * torch.matmul(self.A.T, self.A) + self.Q\n",
    "        lincost_mat = -self.rho * torch.matmul(b, self.A).T + self.q - lamda\n",
    "        lincost_mat = lincost_mat.view(-1, 1)\n",
    "        sol, _ = torch.solve(lincost_mat, -cost_mat)\n",
    "        sol = sol.view(-1)\n",
    "        res = torch.matmul(self.A, sol) - b\n",
    "        return sol, res\n",
    "    \n",
    "    def optimize(self, b, lamda):\n",
    "        sol, res = self.compute_augmented_lagrangian(b, lamda)\n",
    "        for i in range(0, self.maxiter):\n",
    "            sol, res = self.compute_augmented_lagrangian(b, lamda)\n",
    "            lamda -= self.rho * torch.matmul(self.A.T, res)\n",
    "        return sol\n",
    "    \n",
    "    def solve(self, b, lamda):\n",
    "        batch_size, _ = b.size()\n",
    "        y = torch.zeros(batch_size, 22, dtype=torch.double).to(device)\n",
    "        for i in range(batch_size):\n",
    "            b_cur = b[i]\n",
    "            lamda_cur = lamda[i]\n",
    "            sol = self.optimize(b_cur, lamda_cur)\n",
    "            y[i, :] = sol\n",
    "        return y, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPFunction(torch.autograd.Function):\n",
    "    \"\"\"Generic declarative autograd function.\n",
    "    Defines the forward and backward functions. Saves all inputs and outputs,\n",
    "    which may be memory-inefficient for the specific problem.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, problem, *inputs):\n",
    "        output, solve_ctx = torch.no_grad()(problem.solve)(*inputs)\n",
    "        ctx.save_for_backward(output, *inputs)\n",
    "        ctx.problem = problem\n",
    "        ctx.solve_ctx = solve_ctx\n",
    "        return output.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, *inputs = ctx.saved_tensors\n",
    "        problem = ctx.problem\n",
    "        solve_ctx = ctx.solve_ctx\n",
    "        output.requires_grad = True\n",
    "        inputs = tuple(inputs)\n",
    "        grad_inputs = problem.gradient(*inputs, y=output, v=grad_output,\n",
    "            ctx=solve_ctx)\n",
    "        return (None, *grad_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeclarativeLayer(torch.nn.Module):\n",
    "    \"\"\"Generic declarative layer.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    Usage:\n",
    "        problem = <derived class of *DeclarativeNode>\n",
    "        declarative_layer = DeclarativeLayer(problem)\n",
    "        y = declarative_layer(x1, x2, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        super(DeclarativeLayer, self).__init__()\n",
    "        self.problem = problem\n",
    "        \n",
    "    def forward(self, *inputs):\n",
    "        return QPFunction.apply(self.problem, *inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrajNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajNet(nn.Module):\n",
    "    def __init__(self, opt_layer, P, input_size=16, hidden_size=64, output_size=12, nvar=11, t_obs=8):\n",
    "        super(TrajNet, self).__init__()\n",
    "        self.nvar = nvar\n",
    "        self.t_obs = t_obs\n",
    "        self.P = torch.tensor(P, dtype=torch.double).to(device)\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.opt_layer = opt_layer\n",
    "        self.activation = nn.ReLU()\n",
    "        self.mask = torch.tensor([[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]], dtype=torch.double).to(device)\n",
    "    \n",
    "    def forward(self, x, b):\n",
    "        batch_size, _, _ = x.size()\n",
    "        out = self.activation(self.linear1(x))\n",
    "        _, (hn, cn) = self.lstm(out)\n",
    "        b_pred = self.linear2(hn[0])\n",
    "        b_gen = self.mask * b + (1 - self.mask) * b_pred\n",
    "        \n",
    "        # Run optimization\n",
    "        lamda = torch.zeros(batch_size, 2 * self.nvar, dtype=torch.double).to(device)\n",
    "        sol = self.opt_layer(b_gen, lamda)\n",
    "        \n",
    "        # Compute final trajectory\n",
    "        x_pred = torch.matmul(self.P, sol[:, :self.nvar].transpose(0, 1))[self.t_obs:]\n",
    "        y_pred = torch.matmul(self.P, sol[:, self.nvar:].transpose(0, 1))[self.t_obs:]\n",
    "        \n",
    "        x_pred = x_pred.transpose(0, 1)\n",
    "        y_pred = y_pred.transpose(0, 1)\n",
    "        out = torch.cat([x_pred, y_pred], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectory Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, root_dir, t_obs=8):\n",
    "        self.root_dir = root_dir\n",
    "        self.t_obs = t_obs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = \"{}.npy\".format(idx)\n",
    "        file_path = os.path.join(self.root_dir, file_name)\n",
    "        \n",
    "        data = np.load(file_path, allow_pickle=True).item()\n",
    "        x_traj = data['x_traj']\n",
    "        y_traj = data['y_traj']\n",
    "        \n",
    "        x_inp = x_traj[:self.t_obs]\n",
    "        y_inp = y_traj[:self.t_obs]\n",
    "        x_fut = x_traj[self.t_obs:]\n",
    "        y_fut = y_traj[self.t_obs:]\n",
    "\n",
    "        traj_inp = np.dstack((x_inp, y_inp)).squeeze(0)\n",
    "        traj_out = np.hstack((x_fut, y_fut)).flatten()\n",
    "        b_inp = np.array([data['x_init'], data['vx_init'], data['ax_init'], 0, 0, 0, data['y_init'], data['vy_init'], data['ay_init'], 0, 0, 0])\n",
    "        \n",
    "        return torch.tensor(traj_inp), torch.tensor(traj_out), torch.tensor(b_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrajectoryDataset(\"../datasets/data/\", 8)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrajectoryDataset(\"../datasets1/data/\", 8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_num, data in enumerate(train_loader):\n",
    "    traj_inp, traj_out, b_inp = data\n",
    "    print(traj_inp.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = QPNode(Q_np, q_np, A_eq_np)\n",
    "qp_layer = DeclarativeLayer(problem)\n",
    "\n",
    "model = TrajNet(qp_layer, P, input_size=2)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 38.426858541080826\n",
      "Epoch: 0, Batch: 10, Loss: 50.364212205307545\n",
      "Epoch: 0, Batch: 20, Loss: 46.58362537732284\n",
      "Epoch: 0, Batch: 30, Loss: 52.201677550212345\n",
      "Epoch: 0, Batch: 40, Loss: 43.0479747282934\n",
      "Epoch: 0, Mean Loss: 43.60427045454773\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Batch: 0, Loss: 46.80242010415944\n",
      "Epoch: 1, Batch: 10, Loss: 41.63375428107997\n",
      "Epoch: 1, Batch: 20, Loss: 50.56669515250226\n",
      "Epoch: 1, Batch: 30, Loss: 46.32481811539659\n",
      "Epoch: 1, Batch: 40, Loss: 39.3124684166278\n",
      "Epoch: 1, Mean Loss: 41.383286588280036\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Batch: 0, Loss: 40.942905423318095\n",
      "Epoch: 2, Batch: 10, Loss: 48.253521278426575\n",
      "Epoch: 2, Batch: 20, Loss: 33.54580310072001\n",
      "Epoch: 2, Batch: 30, Loss: 39.215288754054086\n",
      "Epoch: 2, Batch: 40, Loss: 56.78653842796498\n",
      "Epoch: 2, Mean Loss: 39.5975899734879\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Batch: 0, Loss: 51.606795604668\n",
      "Epoch: 3, Batch: 10, Loss: 35.68605479718825\n",
      "Epoch: 3, Batch: 20, Loss: 40.431654226874386\n",
      "Epoch: 3, Batch: 30, Loss: 39.70036673305042\n",
      "Epoch: 3, Batch: 40, Loss: 34.01465479307205\n",
      "Epoch: 3, Mean Loss: 37.156131569425\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Batch: 0, Loss: 39.18115266523957\n",
      "Epoch: 4, Batch: 10, Loss: 32.01738468442846\n",
      "Epoch: 4, Batch: 20, Loss: 39.594569292317466\n",
      "Epoch: 4, Batch: 30, Loss: 36.822043748570664\n",
      "Epoch: 4, Batch: 40, Loss: 29.25356991431607\n",
      "Epoch: 4, Mean Loss: 34.229921193138765\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Batch: 0, Loss: 37.31105863928099\n",
      "Epoch: 5, Batch: 10, Loss: 38.89017491758311\n",
      "Epoch: 5, Batch: 20, Loss: 31.637501318709194\n",
      "Epoch: 5, Batch: 30, Loss: 28.69092750560005\n",
      "Epoch: 5, Batch: 40, Loss: 22.27922372321203\n",
      "Epoch: 5, Mean Loss: 31.4102186042372\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6, Batch: 0, Loss: 31.650985853299947\n",
      "Epoch: 6, Batch: 10, Loss: 22.801614021437732\n",
      "Epoch: 6, Batch: 20, Loss: 26.832305407321957\n",
      "Epoch: 6, Batch: 30, Loss: 25.821979889537182\n",
      "Epoch: 6, Batch: 40, Loss: 19.2693233608671\n",
      "Epoch: 6, Mean Loss: 27.535227685474197\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7, Batch: 0, Loss: 18.558619447162023\n",
      "Epoch: 7, Batch: 10, Loss: 23.563905086899027\n",
      "Epoch: 7, Batch: 20, Loss: 18.619154097404948\n",
      "Epoch: 7, Batch: 30, Loss: 18.11861593593113\n",
      "Epoch: 7, Batch: 40, Loss: 27.49245029901935\n",
      "Epoch: 7, Mean Loss: 24.03857853125198\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8, Batch: 0, Loss: 18.20133020851939\n",
      "Epoch: 8, Batch: 10, Loss: 20.78260927085735\n",
      "Epoch: 8, Batch: 20, Loss: 17.035175943207538\n",
      "Epoch: 8, Batch: 30, Loss: 17.768030021972915\n",
      "Epoch: 8, Batch: 40, Loss: 20.354463382150197\n",
      "Epoch: 8, Mean Loss: 22.18192408807873\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9, Batch: 0, Loss: 25.508698296904406\n",
      "Epoch: 9, Batch: 10, Loss: 16.94180757293864\n",
      "Epoch: 9, Batch: 20, Loss: 17.57792066820697\n",
      "Epoch: 9, Batch: 30, Loss: 15.783623370207314\n",
      "Epoch: 9, Batch: 40, Loss: 28.148169081931226\n",
      "Epoch: 9, Mean Loss: 19.287974952509774\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10, Batch: 0, Loss: 18.719311061168135\n",
      "Epoch: 10, Batch: 10, Loss: 16.021775141083236\n",
      "Epoch: 10, Batch: 20, Loss: 36.61882931654732\n",
      "Epoch: 10, Batch: 30, Loss: 21.816724046827183\n",
      "Epoch: 10, Batch: 40, Loss: 18.375164887081628\n",
      "Epoch: 10, Mean Loss: 18.19729345659883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11, Batch: 0, Loss: 13.933603104747533\n",
      "Epoch: 11, Batch: 10, Loss: 15.799825201174997\n",
      "Epoch: 11, Batch: 20, Loss: 11.820402539989681\n",
      "Epoch: 11, Batch: 30, Loss: 19.117907247062288\n",
      "Epoch: 11, Batch: 40, Loss: 10.680321779032193\n",
      "Epoch: 11, Mean Loss: 14.850002198529335\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12, Batch: 0, Loss: 18.958757766645036\n",
      "Epoch: 12, Batch: 10, Loss: 15.61734901890365\n",
      "Epoch: 12, Batch: 20, Loss: 10.965111993010744\n",
      "Epoch: 12, Batch: 30, Loss: 13.627929310665554\n",
      "Epoch: 12, Batch: 40, Loss: 11.440490188746626\n",
      "Epoch: 12, Mean Loss: 13.148046181487423\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13, Batch: 0, Loss: 12.225412225499879\n",
      "Epoch: 13, Batch: 10, Loss: 19.464802750447454\n",
      "Epoch: 13, Batch: 20, Loss: 10.449237024927474\n",
      "Epoch: 13, Batch: 30, Loss: 16.383342839272398\n",
      "Epoch: 13, Batch: 40, Loss: 8.202859594328157\n",
      "Epoch: 13, Mean Loss: 12.690388277212996\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14, Batch: 0, Loss: 8.930116932002148\n",
      "Epoch: 14, Batch: 10, Loss: 14.048645724161341\n",
      "Epoch: 14, Batch: 20, Loss: 10.312995695581288\n",
      "Epoch: 14, Batch: 30, Loss: 7.611963621101436\n",
      "Epoch: 14, Batch: 40, Loss: 11.757046710566422\n",
      "Epoch: 14, Mean Loss: 12.100920804754672\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15, Batch: 0, Loss: 9.683362457414102\n",
      "Epoch: 15, Batch: 10, Loss: 12.054612168026468\n",
      "Epoch: 15, Batch: 20, Loss: 5.845810320113123\n",
      "Epoch: 15, Batch: 30, Loss: 9.268541528059245\n",
      "Epoch: 15, Batch: 40, Loss: 11.331760128982575\n",
      "Epoch: 15, Mean Loss: 8.994029977394455\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16, Batch: 0, Loss: 6.553826539833125\n",
      "Epoch: 16, Batch: 10, Loss: 11.346389124624517\n",
      "Epoch: 16, Batch: 20, Loss: 7.495330472175374\n",
      "Epoch: 16, Batch: 30, Loss: 11.340655228428258\n",
      "Epoch: 16, Batch: 40, Loss: 6.219073806031209\n",
      "Epoch: 16, Mean Loss: 8.450830064665908\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17, Batch: 0, Loss: 4.825457320542578\n",
      "Epoch: 17, Batch: 10, Loss: 5.967418056587218\n",
      "Epoch: 17, Batch: 20, Loss: 8.489884149380797\n",
      "Epoch: 17, Batch: 30, Loss: 8.26376090602709\n",
      "Epoch: 17, Batch: 40, Loss: 6.55182367593667\n",
      "Epoch: 17, Mean Loss: 7.384298585755784\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18, Batch: 0, Loss: 6.12962255548875\n",
      "Epoch: 18, Batch: 10, Loss: 7.556523131861444\n",
      "Epoch: 18, Batch: 20, Loss: 4.716649257929305\n",
      "Epoch: 18, Batch: 30, Loss: 4.280977696687256\n",
      "Epoch: 18, Batch: 40, Loss: 8.15771094388709\n",
      "Epoch: 18, Mean Loss: 7.014782015458259\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19, Batch: 0, Loss: 6.5068659077573106\n",
      "Epoch: 19, Batch: 10, Loss: 7.80285367477964\n",
      "Epoch: 19, Batch: 20, Loss: 6.920435961592276\n",
      "Epoch: 19, Batch: 30, Loss: 3.73291829701008\n",
      "Epoch: 19, Batch: 40, Loss: 6.428860200741694\n",
      "Epoch: 19, Mean Loss: 6.797747862043949\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20, Batch: 0, Loss: 4.329422219459527\n",
      "Epoch: 20, Batch: 10, Loss: 5.7463592446570795\n",
      "Epoch: 20, Batch: 20, Loss: 4.877913180735747\n",
      "Epoch: 20, Batch: 30, Loss: 6.60233396586817\n",
      "Epoch: 20, Batch: 40, Loss: 3.1993128635158357\n",
      "Epoch: 20, Mean Loss: 5.388660071046229\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21, Batch: 0, Loss: 2.4155754702912136\n",
      "Epoch: 21, Batch: 10, Loss: 4.826057029910946\n",
      "Epoch: 21, Batch: 20, Loss: 4.219691243948203\n",
      "Epoch: 21, Batch: 30, Loss: 5.229195402041494\n",
      "Epoch: 21, Batch: 40, Loss: 6.365611724780737\n",
      "Epoch: 21, Mean Loss: 5.032280628186635\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22, Batch: 0, Loss: 4.848253436622415\n",
      "Epoch: 22, Batch: 10, Loss: 3.803621523155104\n",
      "Epoch: 22, Batch: 20, Loss: 4.488996941372121\n",
      "Epoch: 22, Batch: 30, Loss: 5.092734755860524\n",
      "Epoch: 22, Batch: 40, Loss: 4.309375665427138\n",
      "Epoch: 22, Mean Loss: 5.229721990962281\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23, Batch: 0, Loss: 5.911616561955048\n",
      "Epoch: 23, Batch: 10, Loss: 2.9014003352591873\n",
      "Epoch: 23, Batch: 20, Loss: 4.644418420778842\n",
      "Epoch: 23, Batch: 30, Loss: 3.837125377192004\n",
      "Epoch: 23, Batch: 40, Loss: 3.7694435705282823\n",
      "Epoch: 23, Mean Loss: 4.619159128946426\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24, Batch: 0, Loss: 4.48159126602167\n",
      "Epoch: 24, Batch: 10, Loss: 3.4275227769140573\n",
      "Epoch: 24, Batch: 20, Loss: 3.3469495200311097\n",
      "Epoch: 24, Batch: 30, Loss: 3.2335702890964098\n",
      "Epoch: 24, Batch: 40, Loss: 5.12875056156184\n",
      "Epoch: 24, Mean Loss: 4.287854138118905\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 25, Batch: 0, Loss: 4.464990771192536\n",
      "Epoch: 25, Batch: 10, Loss: 3.3336554604655904\n",
      "Epoch: 25, Batch: 20, Loss: 3.1474699589783284\n",
      "Epoch: 25, Batch: 30, Loss: 3.0121251855071334\n",
      "Epoch: 25, Batch: 40, Loss: 4.053092692453492\n",
      "Epoch: 25, Mean Loss: 4.223697067141714\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 26, Batch: 0, Loss: 4.659484717581377\n",
      "Epoch: 26, Batch: 10, Loss: 3.572061158693627\n",
      "Epoch: 26, Batch: 20, Loss: 4.392245591924936\n",
      "Epoch: 26, Batch: 30, Loss: 4.925858904325124\n",
      "Epoch: 26, Batch: 40, Loss: 5.030451343564944\n",
      "Epoch: 26, Mean Loss: 4.097434645145402\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 27, Batch: 0, Loss: 4.216776154269938\n",
      "Epoch: 27, Batch: 10, Loss: 4.641804874454787\n",
      "Epoch: 27, Batch: 20, Loss: 2.4224249711152774\n",
      "Epoch: 27, Batch: 30, Loss: 4.456699523597521\n",
      "Epoch: 27, Batch: 40, Loss: 2.7741865035686972\n",
      "Epoch: 27, Mean Loss: 3.597618652670795\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 28, Batch: 0, Loss: 2.5360098272962195\n",
      "Epoch: 28, Batch: 10, Loss: 3.776366084122454\n",
      "Epoch: 28, Batch: 20, Loss: 2.556573356078299\n",
      "Epoch: 28, Batch: 30, Loss: 2.5636276388008783\n",
      "Epoch: 28, Batch: 40, Loss: 2.8503374438307008\n",
      "Epoch: 28, Mean Loss: 3.4621294814750927\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 29, Batch: 0, Loss: 2.335314283456476\n",
      "Epoch: 29, Batch: 10, Loss: 3.912359550683242\n",
      "Epoch: 29, Batch: 20, Loss: 3.3613345018142167\n",
      "Epoch: 29, Batch: 30, Loss: 2.8168101161610326\n",
      "Epoch: 29, Batch: 40, Loss: 2.5461741310497588\n",
      "Epoch: 29, Mean Loss: 3.3323920093966684\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 30, Batch: 0, Loss: 1.6092578766315127\n",
      "Epoch: 30, Batch: 10, Loss: 2.700369988541275\n",
      "Epoch: 30, Batch: 20, Loss: 5.2974587408992155\n",
      "Epoch: 30, Batch: 30, Loss: 3.1528407612511744\n",
      "Epoch: 30, Batch: 40, Loss: 6.229541847149111\n",
      "Epoch: 30, Mean Loss: 3.629723609855465\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 31, Batch: 0, Loss: 2.5934149107911364\n",
      "Epoch: 31, Batch: 10, Loss: 3.149724637356321\n",
      "Epoch: 31, Batch: 20, Loss: 2.735487560106287\n",
      "Epoch: 31, Batch: 30, Loss: 3.540963034803037\n",
      "Epoch: 31, Batch: 40, Loss: 3.320089524122713\n",
      "Epoch: 31, Mean Loss: 3.105612124301986\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 32, Batch: 0, Loss: 2.6026683997526296\n",
      "Epoch: 32, Batch: 10, Loss: 2.4699591883115106\n",
      "Epoch: 32, Batch: 20, Loss: 3.6079175053275994\n",
      "Epoch: 32, Batch: 30, Loss: 3.690216668551447\n",
      "Epoch: 32, Batch: 40, Loss: 2.321093513141716\n",
      "Epoch: 32, Mean Loss: 2.6423577452769784\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 33, Batch: 0, Loss: 3.0866730782062537\n",
      "Epoch: 33, Batch: 10, Loss: 3.229244915185895\n",
      "Epoch: 33, Batch: 20, Loss: 3.3804898347906414\n",
      "Epoch: 33, Batch: 30, Loss: 2.9171189314425447\n",
      "Epoch: 33, Batch: 40, Loss: 2.5152821697505137\n",
      "Epoch: 33, Mean Loss: 2.671122819171772\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 34, Batch: 0, Loss: 2.340056448735402\n",
      "Epoch: 34, Batch: 10, Loss: 1.923850317790241\n",
      "Epoch: 34, Batch: 20, Loss: 1.789354889387962\n",
      "Epoch: 34, Batch: 30, Loss: 2.396951474671268\n",
      "Epoch: 34, Batch: 40, Loss: 2.5120139887541963\n",
      "Epoch: 34, Mean Loss: 2.553401742184382\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 35, Batch: 0, Loss: 1.6967193783647896\n",
      "Epoch: 35, Batch: 10, Loss: 2.4188457165844826\n",
      "Epoch: 35, Batch: 20, Loss: 2.497069558471018\n",
      "Epoch: 35, Batch: 30, Loss: 3.2189753895252218\n",
      "Epoch: 35, Batch: 40, Loss: 4.2272385213917065\n",
      "Epoch: 35, Mean Loss: 2.5567013921580353\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 36, Batch: 0, Loss: 1.5756884726597253\n",
      "Epoch: 36, Batch: 10, Loss: 2.12213822855163\n",
      "Epoch: 36, Batch: 20, Loss: 2.63995322603171\n",
      "Epoch: 36, Batch: 30, Loss: 1.9085348330774603\n",
      "Epoch: 36, Batch: 40, Loss: 1.8150472505469368\n",
      "Epoch: 36, Mean Loss: 2.360462307935664\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 37, Batch: 0, Loss: 2.229860986377244\n",
      "Epoch: 37, Batch: 10, Loss: 2.957370533223057\n",
      "Epoch: 37, Batch: 20, Loss: 1.9249031542478956\n",
      "Epoch: 37, Batch: 30, Loss: 1.8504566523088868\n",
      "Epoch: 37, Batch: 40, Loss: 2.415368214893518\n",
      "Epoch: 37, Mean Loss: 2.4851899788655736\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 38, Batch: 0, Loss: 2.5130613718061956\n",
      "Epoch: 38, Batch: 10, Loss: 2.7860993484686447\n",
      "Epoch: 38, Batch: 20, Loss: 2.053764833292664\n",
      "Epoch: 38, Batch: 30, Loss: 2.2312325698706053\n",
      "Epoch: 38, Batch: 40, Loss: 2.8294531051824805\n",
      "Epoch: 38, Mean Loss: 2.0244754265227902\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 39, Batch: 0, Loss: 2.157207620231016\n",
      "Epoch: 39, Batch: 10, Loss: 1.8075262314556584\n",
      "Epoch: 39, Batch: 20, Loss: 2.1205774764591463\n",
      "Epoch: 39, Batch: 30, Loss: 1.506547459371434\n",
      "Epoch: 39, Batch: 40, Loss: 1.9417835474678975\n",
      "Epoch: 39, Mean Loss: 1.9918673351857585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 40, Batch: 0, Loss: 1.5392108470112291\n",
      "Epoch: 40, Batch: 10, Loss: 1.8417454160321307\n",
      "Epoch: 40, Batch: 20, Loss: 3.9637924631038297\n",
      "Epoch: 40, Batch: 30, Loss: 2.248539579553581\n",
      "Epoch: 40, Batch: 40, Loss: 2.120568899886124\n",
      "Epoch: 40, Mean Loss: 2.062484247090784\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 41, Batch: 0, Loss: 1.4976193767071682\n",
      "Epoch: 41, Batch: 10, Loss: 1.7376911759841862\n",
      "Epoch: 41, Batch: 20, Loss: 1.149444912728488\n",
      "Epoch: 41, Batch: 30, Loss: 1.9037764754376696\n",
      "Epoch: 41, Batch: 40, Loss: 2.2695419207207124\n",
      "Epoch: 41, Mean Loss: 1.8556812413151837\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 42, Batch: 0, Loss: 1.5041819068679094\n",
      "Epoch: 42, Batch: 10, Loss: 2.453617150397305\n",
      "Epoch: 42, Batch: 20, Loss: 1.831191992072534\n",
      "Epoch: 42, Batch: 30, Loss: 1.6450313344204477\n",
      "Epoch: 42, Batch: 40, Loss: 1.7514573432177536\n",
      "Epoch: 42, Mean Loss: 1.8744676435156082\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 43, Batch: 0, Loss: 1.5406343900351345\n",
      "Epoch: 43, Batch: 10, Loss: 1.9845358049392052\n",
      "Epoch: 43, Batch: 20, Loss: 1.3747306009269884\n",
      "Epoch: 43, Batch: 30, Loss: 2.1843256363129266\n",
      "Epoch: 43, Batch: 40, Loss: 2.9196464331237055\n",
      "Epoch: 43, Mean Loss: 1.870975424094234\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 44, Batch: 0, Loss: 1.5516771067801438\n",
      "Epoch: 44, Batch: 10, Loss: 1.5245719742960262\n",
      "Epoch: 44, Batch: 20, Loss: 1.112017457193372\n",
      "Epoch: 44, Batch: 30, Loss: 1.1928004563233552\n",
      "Epoch: 44, Batch: 40, Loss: 1.4426594384442253\n",
      "Epoch: 44, Mean Loss: 1.7677098520059462\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 45, Batch: 0, Loss: 2.0688571084357728\n",
      "Epoch: 45, Batch: 10, Loss: 1.239853462652688\n",
      "Epoch: 45, Batch: 20, Loss: 1.3520582974912743\n",
      "Epoch: 45, Batch: 30, Loss: 2.2109990956742194\n",
      "Epoch: 45, Batch: 40, Loss: 2.438593022623138\n",
      "Epoch: 45, Mean Loss: 1.8434008162971596\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 46, Batch: 0, Loss: 1.390443091045113\n",
      "Epoch: 46, Batch: 10, Loss: 1.1322657091899682\n",
      "Epoch: 46, Batch: 20, Loss: 1.6621536021887\n",
      "Epoch: 46, Batch: 30, Loss: 1.7960087697704827\n",
      "Epoch: 46, Batch: 40, Loss: 0.9412972802877634\n",
      "Epoch: 46, Mean Loss: 1.5587279535099987\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 47, Batch: 0, Loss: 1.5065181413270492\n",
      "Epoch: 47, Batch: 10, Loss: 1.6157872765776584\n",
      "Epoch: 47, Batch: 20, Loss: 1.423083351128295\n",
      "Epoch: 47, Batch: 30, Loss: 1.0749404806139387\n",
      "Epoch: 47, Batch: 40, Loss: 1.231614049651108\n",
      "Epoch: 47, Mean Loss: 1.4445329136302323\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 48, Batch: 0, Loss: 1.4213357755352958\n",
      "Epoch: 48, Batch: 10, Loss: 1.2689130698030198\n",
      "Epoch: 48, Batch: 20, Loss: 1.3650934919012159\n",
      "Epoch: 48, Batch: 30, Loss: 2.090849674082518\n",
      "Epoch: 48, Batch: 40, Loss: 1.2654488265143207\n",
      "Epoch: 48, Mean Loss: 1.3968493324535067\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 49, Batch: 0, Loss: 1.4658959642928626\n",
      "Epoch: 49, Batch: 10, Loss: 1.2315636082936408\n",
      "Epoch: 49, Batch: 20, Loss: 1.4399949201015052\n",
      "Epoch: 49, Batch: 30, Loss: 1.1024369035121568\n",
      "Epoch: 49, Batch: 40, Loss: 1.5501816655671454\n",
      "Epoch: 49, Mean Loss: 1.4343498064737685\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for batch_num, data in enumerate(train_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        if batch_num % 10 == 0:\n",
    "            print(\"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, batch_num, loss.item()))\n",
    "    \n",
    "    mean_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(mean_loss)\n",
    "    print(\"Epoch: {}, Mean Loss: {}\".format(epoch, mean_loss))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_inp[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(i, traj_inp, traj_out, traj_pred):\n",
    "    traj_inp = traj_inp.numpy()\n",
    "    traj_out = traj_out.numpy()\n",
    "    traj_pred = traj_pred.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(traj_inp[:, 0], traj_inp[:, 1], label='Inp traj')\n",
    "    ax.scatter(traj_out[:12], traj_out[12:], label='GT')\n",
    "    ax.scatter(traj_pred[:12], traj_pred[12:], label='Pred')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-20, 20])\n",
    "    ax.set_ylim([-20, 20])\n",
    "    plt.savefig('./results_lstm/{}.png'.format(i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 1.436795499431562\n",
      "Batch: 1, Loss: 1.7719794245410592\n",
      "Batch: 2, Loss: 1.6644465699198518\n",
      "Batch: 3, Loss: 1.845984843209607\n",
      "Batch: 4, Loss: 1.6577661488449376\n",
      "Batch: 5, Loss: 1.479331258899557\n",
      "Batch: 6, Loss: 1.0856660339854898\n",
      "Batch: 7, Loss: 1.2323607246831547\n",
      "Batch: 8, Loss: 1.5252875066027454\n",
      "Batch: 9, Loss: 2.1512220968681275\n",
      "Batch: 10, Loss: 1.23626468736946\n",
      "Batch: 11, Loss: 1.987946175071933\n",
      "Batch: 12, Loss: 1.1453004874205683\n",
      "Batch: 13, Loss: 1.116795353216222\n",
      "Batch: 14, Loss: 1.3950119543003505\n",
      "Batch: 15, Loss: 0.9941272424898908\n",
      "Batch: 16, Loss: 1.5626530022307363\n",
      "Batch: 17, Loss: 1.1818309932601856\n",
      "Batch: 18, Loss: 1.6742237905726667\n",
      "Batch: 19, Loss: 2.023394889425496\n",
      "Batch: 20, Loss: 2.4761343390849495\n",
      "Batch: 21, Loss: 1.2468824851507754\n",
      "Batch: 22, Loss: 1.879981122621146\n",
      "Batch: 23, Loss: 1.627572501396488\n",
      "Batch: 24, Loss: 1.2095810515481324\n",
      "Batch: 25, Loss: 1.5431614513450598\n",
      "Batch: 26, Loss: 2.4244796693254274\n",
      "Batch: 27, Loss: 1.7016399836945968\n",
      "Batch: 28, Loss: 0.9924959595183781\n",
      "Batch: 29, Loss: 0.930779680411707\n",
      "Batch: 30, Loss: 1.8087835631323261\n",
      "Batch: 31, Loss: 1.4642784009041094\n",
      "Batch: 32, Loss: 1.3867196392057375\n",
      "Batch: 33, Loss: 1.0602416926212492\n",
      "Batch: 34, Loss: 1.5949912217502367\n",
      "Batch: 35, Loss: 1.9983192217205983\n",
      "Batch: 36, Loss: 1.0407694515785657\n",
      "Batch: 37, Loss: 2.0261480520814024\n",
      "Batch: 38, Loss: 1.9807050459812963\n",
      "Batch: 39, Loss: 1.9718726071284043\n",
      "Batch: 40, Loss: 1.5137319489803516\n",
      "Batch: 41, Loss: 1.1728514065178615\n",
      "Batch: 42, Loss: 1.8902146725746127\n",
      "Batch: 43, Loss: 1.1516672776128365\n",
      "Batch: 44, Loss: 1.0962635182727136\n",
      "Batch: 45, Loss: 2.1033516033785724\n",
      "Batch: 46, Loss: 1.0812827708454313\n",
      "Batch: 47, Loss: 1.75639617532354\n",
      "Batch: 48, Loss: 1.4559991922895246\n",
      "Batch: 49, Loss: 2.451126506618285\n",
      "Epoch Mean Test Loss: 1.5641362178991582\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cnt = 0\n",
    "    test_loss = []\n",
    "    for batch_num, data in enumerate(test_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        print(\"Batch: {}, Loss: {}\".format(batch_num, loss.item()))\n",
    "        \n",
    "        for i in range(traj_inp.size()[0]):\n",
    "            plot_traj(cnt, traj_inp[i], traj_out[i], out[i])\n",
    "            cnt += 1\n",
    "\n",
    "mean_loss = np.mean(test_loss)\n",
    "print(\"Epoch Mean Test Loss: {}\".format(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = model(traj_inp, b_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out.size(), traj_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_out[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((test_out[14] - traj_out[14]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(test_out, traj_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
