{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/shashanks./Downloads/Installations/ddn/\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bernstein import bernstein_coeff_order10_new\n",
    "from ddn.pytorch.node import AbstractDeclarativeNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fin = 4.8\n",
    "num = 12\n",
    "\n",
    "tot_time = np.linspace(0.0, t_fin, num)\n",
    "tot_time_copy = tot_time.reshape(num, 1)\n",
    "P, Pdot, Pddot = bernstein_coeff_order10_new(10, tot_time_copy[0], tot_time_copy[-1], tot_time_copy)\n",
    "nvar = np.shape(P)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_eq_mat = np.vstack((P[0], Pdot[0], Pddot[0], P[-1], Pdot[-1], Pddot[-1]))\n",
    "A_eq_np = block_diag(A_eq_mat, A_eq_mat)\n",
    "Q_np = 10 * block_diag(np.dot(Pddot.T, Pddot), np.dot(Pddot.T, Pddot))\n",
    "q_np = np.zeros(2 * nvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QPNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPNode(AbstractDeclarativeNode):\n",
    "    def __init__(self, Q_np, q_np, A_eq_np, rho=1.0, nvar=22, maxiter=1000):\n",
    "        super().__init__()\n",
    "        self.rho = rho\n",
    "        self.nvar = nvar\n",
    "        self.maxiter = maxiter\n",
    "        self.Q = torch.tensor(Q_np, dtype=torch.double).to(device)\n",
    "        self.q = torch.tensor(q_np, dtype=torch.double).to(device)\n",
    "        self.A = torch.tensor(A_eq_np, dtype=torch.double).to(device)\n",
    "    \n",
    "    def objective(self, b, lamda, y):\n",
    "        \"\"\"\n",
    "        b: (B x 12)\n",
    "        lamda: (B x 22)\n",
    "        y: (B x 22)\n",
    "        \"\"\"\n",
    "        lamda = lamda.transpose(0, 1)\n",
    "        y = y.transpose(0, 1)\n",
    "        cost_mat = self.rho * torch.matmul(self.A.T, self.A) + self.Q\n",
    "        lincost_mat = -self.rho * torch.matmul(b, self.A).T + self.q.view(-1, 1) - lamda\n",
    "        f = 0.5 * torch.diag(torch.matmul(y.T, torch.matmul(cost_mat, y))) + torch.diag(torch.matmul(lincost_mat.T, y))\n",
    "        return f\n",
    "    \n",
    "    def compute_augmented_lagrangian(self, b, lamda):\n",
    "        \"\"\"\n",
    "        b: (12,)\n",
    "        lamda: (22,)\n",
    "        \"\"\"\n",
    "        cost_mat = self.rho * torch.matmul(self.A.T, self.A) + self.Q\n",
    "        lincost_mat = -self.rho * torch.matmul(b, self.A).T + self.q - lamda\n",
    "        lincost_mat = lincost_mat.view(-1, 1)\n",
    "        sol, _ = torch.solve(lincost_mat, -cost_mat)\n",
    "        sol = sol.view(-1)\n",
    "        res = torch.matmul(self.A, sol) - b\n",
    "        return sol, res\n",
    "    \n",
    "    def optimize(self, b, lamda):\n",
    "        sol, res = self.compute_augmented_lagrangian(b, lamda)\n",
    "        for i in range(0, self.maxiter):\n",
    "            sol, res = self.compute_augmented_lagrangian(b, lamda)\n",
    "            lamda -= self.rho * torch.matmul(self.A.T, res)\n",
    "        return sol\n",
    "    \n",
    "    def solve(self, b, lamda):\n",
    "        batch_size, _ = b.size()\n",
    "        y = torch.zeros(batch_size, 22, dtype=torch.double).to(device)\n",
    "        for i in range(batch_size):\n",
    "            b_cur = b[i]\n",
    "            lamda_cur = lamda[i]\n",
    "            sol = self.optimize(b_cur, lamda_cur)\n",
    "            y[i, :] = sol\n",
    "        return y, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPFunction(torch.autograd.Function):\n",
    "    \"\"\"Generic declarative autograd function.\n",
    "    Defines the forward and backward functions. Saves all inputs and outputs,\n",
    "    which may be memory-inefficient for the specific problem.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, problem, *inputs):\n",
    "        output, solve_ctx = torch.no_grad()(problem.solve)(*inputs)\n",
    "        ctx.save_for_backward(output, *inputs)\n",
    "        ctx.problem = problem\n",
    "        ctx.solve_ctx = solve_ctx\n",
    "        return output.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, *inputs = ctx.saved_tensors\n",
    "        problem = ctx.problem\n",
    "        solve_ctx = ctx.solve_ctx\n",
    "        output.requires_grad = True\n",
    "        inputs = tuple(inputs)\n",
    "        grad_inputs = problem.gradient(*inputs, y=output, v=grad_output,\n",
    "            ctx=solve_ctx)\n",
    "        return (None, *grad_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeclarativeLayer(torch.nn.Module):\n",
    "    \"\"\"Generic declarative layer.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    Usage:\n",
    "        problem = <derived class of *DeclarativeNode>\n",
    "        declarative_layer = DeclarativeLayer(problem)\n",
    "        y = declarative_layer(x1, x2, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        super(DeclarativeLayer, self).__init__()\n",
    "        self.problem = problem\n",
    "        \n",
    "    def forward(self, *inputs):\n",
    "        return QPFunction.apply(self.problem, *inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrajNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajNet(nn.Module):\n",
    "    def __init__(self, opt_layer, P, input_size=16, hidden_size=64, output_size=12, nvar=11, t_obs=8):\n",
    "        super(TrajNet, self).__init__()\n",
    "        self.nvar = nvar\n",
    "        self.t_obs = t_obs\n",
    "        self.P = torch.tensor(P, dtype=torch.double).to(device)\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.opt_layer = opt_layer\n",
    "        self.activation = nn.ReLU()\n",
    "        self.mask = torch.tensor([[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]], dtype=torch.double).to(device)\n",
    "    \n",
    "    def forward(self, x, b):\n",
    "        batch_size, _ = x.size()\n",
    "        out = self.activation(self.linear1(x))\n",
    "        b_pred = self.linear2(out)\n",
    "        b_gen = self.mask * b + (1 - self.mask) * b_pred\n",
    "        \n",
    "        # Run optimization\n",
    "        lamda = torch.zeros(batch_size, 2 * self.nvar, dtype=torch.double).to(device)\n",
    "        sol = self.opt_layer(b_gen, lamda)\n",
    "        \n",
    "        # Compute final trajectory\n",
    "        x_pred = torch.matmul(self.P, sol[:, :self.nvar].transpose(0, 1))\n",
    "        y_pred = torch.matmul(self.P, sol[:, self.nvar:].transpose(0, 1))\n",
    "        \n",
    "        x_pred = x_pred.transpose(0, 1)\n",
    "        y_pred = y_pred.transpose(0, 1)\n",
    "        out = torch.cat([x_pred, y_pred], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectory Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, root_dir, t_obs=8, dt=0.4):\n",
    "        self.root_dir = root_dir\n",
    "        self.t_obs = t_obs\n",
    "        self.dt = dt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "    \n",
    "    def get_vel(self, pos):\n",
    "        return (pos[-1] - pos[-2]) / self.dt\n",
    "    \n",
    "    def get_acc(self, vel):\n",
    "        return (vel[-1] - vel[-2]) / self.dt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = \"{}.npy\".format(idx)\n",
    "        file_path = os.path.join(self.root_dir, file_name)\n",
    "        \n",
    "        data = np.load(file_path, allow_pickle=True).item()\n",
    "        x_traj = data['x_traj']\n",
    "        y_traj = data['y_traj']\n",
    "        \n",
    "        x_inp = x_traj[:self.t_obs]\n",
    "        y_inp = y_traj[:self.t_obs]\n",
    "        x_fut = x_traj[self.t_obs:]\n",
    "        y_fut = y_traj[self.t_obs:]\n",
    "        \n",
    "        vx_beg = (x_inp[-1] - x_inp[-2]) / self.dt\n",
    "        vy_beg = (y_inp[-1] - y_inp[-2]) / self.dt\n",
    "        \n",
    "        vx_beg_prev = (x_inp[-2] - x_inp[-3]) / self.dt\n",
    "        vy_beg_prev = (y_inp[-2] - y_inp[-3]) / self.dt\n",
    "        \n",
    "        ax_beg = (vx_beg - vx_beg_prev) / self.dt\n",
    "        ay_beg = (vy_beg - vy_beg_prev) / self.dt\n",
    "\n",
    "        traj_inp = np.dstack((x_inp, y_inp)).flatten()\n",
    "        traj_out = np.hstack((x_fut, y_fut)).flatten()\n",
    "        b_inp = np.array([x_inp[-1], vx_beg, ax_beg, 0, 0, 0, y_inp[-1], vy_beg, ay_beg, 0, 0, 0])\n",
    "        return torch.tensor(traj_inp), torch.tensor(traj_out), torch.tensor(b_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrajectoryDataset(\"../datasets/data/\", 8)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrajectoryDataset(\"../datasets1/data/\", 8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "for batch_num, data in enumerate(train_loader):\n",
    "    traj_inp, traj_out, b_inp = data\n",
    "    print(traj_inp.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = QPNode(Q_np, q_np, A_eq_np)\n",
    "qp_layer = DeclarativeLayer(problem)\n",
    "\n",
    "model = TrajNet(qp_layer, P)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 33.013079039495516\n",
      "Epoch: 0, Batch: 10, Loss: 29.159764343020683\n",
      "Epoch: 0, Batch: 20, Loss: 23.020107455320634\n",
      "Epoch: 0, Batch: 30, Loss: 15.766522517892419\n",
      "Epoch: 0, Batch: 40, Loss: 11.178599180999472\n",
      "Epoch: 0, Mean Loss: 19.5356222082199\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Batch: 0, Loss: 18.448720579747395\n",
      "Epoch: 1, Batch: 10, Loss: 14.640683673131361\n",
      "Epoch: 1, Batch: 20, Loss: 12.927431889030574\n",
      "Epoch: 1, Batch: 30, Loss: 6.135589737376947\n",
      "Epoch: 1, Batch: 40, Loss: 5.089817243891978\n",
      "Epoch: 1, Mean Loss: 8.627628542286864\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Batch: 0, Loss: 3.894518983964221\n",
      "Epoch: 2, Batch: 10, Loss: 6.02254978380591\n",
      "Epoch: 2, Batch: 20, Loss: 6.378049243983864\n",
      "Epoch: 2, Batch: 30, Loss: 11.613679196017696\n",
      "Epoch: 2, Batch: 40, Loss: 7.266596082304288\n",
      "Epoch: 2, Mean Loss: 6.173086442179863\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Batch: 0, Loss: 6.380590714381724\n",
      "Epoch: 3, Batch: 10, Loss: 3.4154218293625953\n",
      "Epoch: 3, Batch: 20, Loss: 5.448681894382085\n",
      "Epoch: 3, Batch: 30, Loss: 4.534073759626158\n",
      "Epoch: 3, Batch: 40, Loss: 4.615038245505876\n",
      "Epoch: 3, Mean Loss: 5.261770889229918\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Batch: 0, Loss: 5.116564266606114\n",
      "Epoch: 4, Batch: 10, Loss: 8.937023869663305\n",
      "Epoch: 4, Batch: 20, Loss: 4.163208640541424\n",
      "Epoch: 4, Batch: 30, Loss: 3.8734852521620744\n",
      "Epoch: 4, Batch: 40, Loss: 3.4324949502135307\n",
      "Epoch: 4, Mean Loss: 4.258080925910667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Batch: 0, Loss: 4.489848383250659\n",
      "Epoch: 5, Batch: 10, Loss: 3.057045962645714\n",
      "Epoch: 5, Batch: 20, Loss: 2.8368881511313826\n",
      "Epoch: 5, Batch: 30, Loss: 4.429524629288514\n",
      "Epoch: 5, Batch: 40, Loss: 3.9729552309156615\n",
      "Epoch: 5, Mean Loss: 3.6873192195969198\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6, Batch: 0, Loss: 2.67444316165312\n",
      "Epoch: 6, Batch: 10, Loss: 3.9821721669190486\n",
      "Epoch: 6, Batch: 20, Loss: 3.311540343681119\n",
      "Epoch: 6, Batch: 30, Loss: 2.7803178550532994\n",
      "Epoch: 6, Batch: 40, Loss: 2.590540871874466\n",
      "Epoch: 6, Mean Loss: 3.7178523598703883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7, Batch: 0, Loss: 3.2549373690615035\n",
      "Epoch: 7, Batch: 10, Loss: 2.174430913005911\n",
      "Epoch: 7, Batch: 20, Loss: 1.979006873400072\n",
      "Epoch: 7, Batch: 30, Loss: 2.7171623861641208\n",
      "Epoch: 7, Batch: 40, Loss: 5.338238087999217\n",
      "Epoch: 7, Mean Loss: 3.1044627132060674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8, Batch: 0, Loss: 2.7811196644051166\n",
      "Epoch: 8, Batch: 10, Loss: 2.678014459323533\n",
      "Epoch: 8, Batch: 20, Loss: 3.0926405161072\n",
      "Epoch: 8, Batch: 30, Loss: 5.219432261754258\n",
      "Epoch: 8, Batch: 40, Loss: 2.1178455859487584\n",
      "Epoch: 8, Mean Loss: 2.945860504415799\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9, Batch: 0, Loss: 2.2827383492093056\n",
      "Epoch: 9, Batch: 10, Loss: 3.26245244986251\n",
      "Epoch: 9, Batch: 20, Loss: 2.3072007082528074\n",
      "Epoch: 9, Batch: 30, Loss: 1.7039301648985092\n",
      "Epoch: 9, Batch: 40, Loss: 2.9093046961312874\n",
      "Epoch: 9, Mean Loss: 2.7844847275510007\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10, Batch: 0, Loss: 3.4327390718304973\n",
      "Epoch: 10, Batch: 10, Loss: 2.2195939017967112\n",
      "Epoch: 10, Batch: 20, Loss: 4.144827363035548\n",
      "Epoch: 10, Batch: 30, Loss: 3.028290724080918\n",
      "Epoch: 10, Batch: 40, Loss: 2.410498339037374\n",
      "Epoch: 10, Mean Loss: 2.505914991356403\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11, Batch: 0, Loss: 2.4428697457473567\n",
      "Epoch: 11, Batch: 10, Loss: 3.2509932837588993\n",
      "Epoch: 11, Batch: 20, Loss: 2.184306305142907\n",
      "Epoch: 11, Batch: 30, Loss: 1.7298101231395149\n",
      "Epoch: 11, Batch: 40, Loss: 2.199197852573961\n",
      "Epoch: 11, Mean Loss: 2.522088438275458\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12, Batch: 0, Loss: 1.7834591081420543\n",
      "Epoch: 12, Batch: 10, Loss: 1.8227383762340403\n",
      "Epoch: 12, Batch: 20, Loss: 1.588713508598393\n",
      "Epoch: 12, Batch: 30, Loss: 2.3211269287118044\n",
      "Epoch: 12, Batch: 40, Loss: 2.6215215952390114\n",
      "Epoch: 12, Mean Loss: 2.0209947352689697\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13, Batch: 0, Loss: 1.796004649147052\n",
      "Epoch: 13, Batch: 10, Loss: 1.9144107629983234\n",
      "Epoch: 13, Batch: 20, Loss: 1.6839105476406053\n",
      "Epoch: 13, Batch: 30, Loss: 3.066457864322897\n",
      "Epoch: 13, Batch: 40, Loss: 3.6819544477618487\n",
      "Epoch: 13, Mean Loss: 2.2109730105303163\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14, Batch: 0, Loss: 1.604629583861461\n",
      "Epoch: 14, Batch: 10, Loss: 5.306761735860323\n",
      "Epoch: 14, Batch: 20, Loss: 1.4884876999136352\n",
      "Epoch: 14, Batch: 30, Loss: 2.4607944829572106\n",
      "Epoch: 14, Batch: 40, Loss: 1.6949254421932745\n",
      "Epoch: 14, Mean Loss: 2.0151629709049255\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15, Batch: 0, Loss: 1.4726380793437768\n",
      "Epoch: 15, Batch: 10, Loss: 1.2547218308452406\n",
      "Epoch: 15, Batch: 20, Loss: 1.7686125666946153\n",
      "Epoch: 15, Batch: 30, Loss: 0.9073847321376844\n",
      "Epoch: 15, Batch: 40, Loss: 1.8375985597871065\n",
      "Epoch: 15, Mean Loss: 1.5288342909138473\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16, Batch: 0, Loss: 1.9321938548286977\n",
      "Epoch: 16, Batch: 10, Loss: 2.092921104064955\n",
      "Epoch: 16, Batch: 20, Loss: 1.3090912217986481\n",
      "Epoch: 16, Batch: 30, Loss: 2.448527396997841\n",
      "Epoch: 16, Batch: 40, Loss: 1.591417541838105\n",
      "Epoch: 16, Mean Loss: 2.184874380702541\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17, Batch: 0, Loss: 1.9216689659149997\n",
      "Epoch: 17, Batch: 10, Loss: 1.2134947824851803\n",
      "Epoch: 17, Batch: 20, Loss: 2.626398412391398\n",
      "Epoch: 17, Batch: 30, Loss: 1.819278583201328\n",
      "Epoch: 17, Batch: 40, Loss: 1.0036447379134563\n",
      "Epoch: 17, Mean Loss: 1.505532604726293\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18, Batch: 0, Loss: 1.0029540053287629\n",
      "Epoch: 18, Batch: 10, Loss: 3.3116859047355116\n",
      "Epoch: 18, Batch: 20, Loss: 1.1356052634718352\n",
      "Epoch: 18, Batch: 30, Loss: 1.2704215658172766\n",
      "Epoch: 18, Batch: 40, Loss: 1.2150030999119963\n",
      "Epoch: 18, Mean Loss: 1.8379433352139771\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19, Batch: 0, Loss: 2.330708708226047\n",
      "Epoch: 19, Batch: 10, Loss: 1.1345278018630653\n",
      "Epoch: 19, Batch: 20, Loss: 4.269444364374709\n",
      "Epoch: 19, Batch: 30, Loss: 0.9585118345355526\n",
      "Epoch: 19, Batch: 40, Loss: 1.7878475313492113\n",
      "Epoch: 19, Mean Loss: 1.7154447766613266\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20, Batch: 0, Loss: 1.2980118560706189\n",
      "Epoch: 20, Batch: 10, Loss: 0.8592771492859382\n",
      "Epoch: 20, Batch: 20, Loss: 1.471827115600079\n",
      "Epoch: 20, Batch: 30, Loss: 2.209886987644248\n",
      "Epoch: 20, Batch: 40, Loss: 1.0522535761078582\n",
      "Epoch: 20, Mean Loss: 1.2110921049624985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21, Batch: 0, Loss: 0.8587729155234025\n",
      "Epoch: 21, Batch: 10, Loss: 2.553908092168323\n",
      "Epoch: 21, Batch: 20, Loss: 0.8773527135708321\n",
      "Epoch: 21, Batch: 30, Loss: 1.0606250762827278\n",
      "Epoch: 21, Batch: 40, Loss: 1.3110777978937278\n",
      "Epoch: 21, Mean Loss: 1.2528997982282093\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22, Batch: 0, Loss: 0.8752224062108553\n",
      "Epoch: 22, Batch: 10, Loss: 0.8688101118127263\n",
      "Epoch: 22, Batch: 20, Loss: 0.9459350196658626\n",
      "Epoch: 22, Batch: 30, Loss: 0.990136812170108\n",
      "Epoch: 22, Batch: 40, Loss: 2.9820025554274654\n",
      "Epoch: 22, Mean Loss: 1.5126904971015966\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23, Batch: 0, Loss: 1.426480153794413\n",
      "Epoch: 23, Batch: 10, Loss: 3.3394290768224315\n",
      "Epoch: 23, Batch: 20, Loss: 2.080520193547287\n",
      "Epoch: 23, Batch: 30, Loss: 0.8653929979956113\n",
      "Epoch: 23, Batch: 40, Loss: 1.159308136798495\n",
      "Epoch: 23, Mean Loss: 1.3358599377444276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24, Batch: 0, Loss: 0.6428359088858031\n",
      "Epoch: 24, Batch: 10, Loss: 1.2771327349035835\n",
      "Epoch: 24, Batch: 20, Loss: 1.3157342952470605\n",
      "Epoch: 24, Batch: 30, Loss: 0.8689752713312845\n",
      "Epoch: 24, Batch: 40, Loss: 2.341500371256403\n",
      "Epoch: 24, Mean Loss: 1.544903442581169\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 25, Batch: 0, Loss: 1.1781491372616508\n",
      "Epoch: 25, Batch: 10, Loss: 1.721996321626935\n",
      "Epoch: 25, Batch: 20, Loss: 0.792003211426536\n",
      "Epoch: 25, Batch: 30, Loss: 0.6533312346544436\n",
      "Epoch: 25, Batch: 40, Loss: 2.276689925984667\n",
      "Epoch: 25, Mean Loss: 1.3264017750835948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 26, Batch: 0, Loss: 0.705144923008788\n",
      "Epoch: 26, Batch: 10, Loss: 0.5762400936304028\n",
      "Epoch: 26, Batch: 20, Loss: 0.5985654943486581\n",
      "Epoch: 26, Batch: 30, Loss: 0.6903605120545344\n",
      "Epoch: 26, Batch: 40, Loss: 1.7526663683788548\n",
      "Epoch: 26, Mean Loss: 1.2670211504207638\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 27, Batch: 0, Loss: 1.161627971720962\n",
      "Epoch: 27, Batch: 10, Loss: 0.8139072401756683\n",
      "Epoch: 27, Batch: 20, Loss: 1.028305437050944\n",
      "Epoch: 27, Batch: 30, Loss: 1.0913544206512502\n",
      "Epoch: 27, Batch: 40, Loss: 0.6811861855521645\n",
      "Epoch: 27, Mean Loss: 0.8471508391901189\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 28, Batch: 0, Loss: 0.7406446498402706\n",
      "Epoch: 28, Batch: 10, Loss: 0.6115868836772281\n",
      "Epoch: 28, Batch: 20, Loss: 0.5650696009464302\n",
      "Epoch: 28, Batch: 30, Loss: 0.8364264597728854\n",
      "Epoch: 28, Batch: 40, Loss: 1.6888952433636752\n",
      "Epoch: 28, Mean Loss: 0.8278080207579934\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 29, Batch: 0, Loss: 1.3184906879666216\n",
      "Epoch: 29, Batch: 10, Loss: 0.652732028131321\n",
      "Epoch: 29, Batch: 20, Loss: 0.6433564311804695\n",
      "Epoch: 29, Batch: 30, Loss: 3.3506955593834533\n",
      "Epoch: 29, Batch: 40, Loss: 0.7558684301348239\n",
      "Epoch: 29, Mean Loss: 1.244428819898285\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 30, Batch: 0, Loss: 0.9657839237016783\n",
      "Epoch: 30, Batch: 10, Loss: 0.7664429284754765\n",
      "Epoch: 30, Batch: 20, Loss: 0.4805713608590972\n",
      "Epoch: 30, Batch: 30, Loss: 1.2042581146209443\n",
      "Epoch: 30, Batch: 40, Loss: 1.5396268943837983\n",
      "Epoch: 30, Mean Loss: 0.8739369923336346\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 31, Batch: 0, Loss: 0.7062325774258198\n",
      "Epoch: 31, Batch: 10, Loss: 0.7154593633102436\n",
      "Epoch: 31, Batch: 20, Loss: 1.1038948905247423\n",
      "Epoch: 31, Batch: 30, Loss: 0.9391984294552638\n",
      "Epoch: 31, Batch: 40, Loss: 1.368511864928121\n",
      "Epoch: 31, Mean Loss: 0.916542353649528\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 32, Batch: 0, Loss: 1.7832235723435776\n",
      "Epoch: 32, Batch: 10, Loss: 2.71352218666219\n",
      "Epoch: 32, Batch: 20, Loss: 0.6858307766493066\n",
      "Epoch: 32, Batch: 30, Loss: 1.0937394163348715\n",
      "Epoch: 32, Batch: 40, Loss: 0.7344339639371298\n",
      "Epoch: 32, Mean Loss: 1.2846188851973324\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 33, Batch: 0, Loss: 0.6345829037005418\n",
      "Epoch: 33, Batch: 10, Loss: 0.8357514110644387\n",
      "Epoch: 33, Batch: 20, Loss: 1.323193798734059\n",
      "Epoch: 33, Batch: 30, Loss: 0.63914480175341\n",
      "Epoch: 33, Batch: 40, Loss: 0.5342386568460094\n",
      "Epoch: 33, Mean Loss: 0.8476599316955918\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 34, Batch: 0, Loss: 0.7128934628413497\n",
      "Epoch: 34, Batch: 10, Loss: 0.7850063641988376\n",
      "Epoch: 34, Batch: 20, Loss: 0.7752224366163226\n",
      "Epoch: 34, Batch: 30, Loss: 0.7044332614253903\n",
      "Epoch: 34, Batch: 40, Loss: 0.8152596073437434\n",
      "Epoch: 34, Mean Loss: 0.7592180314195386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 35, Batch: 0, Loss: 1.4911188377792863\n",
      "Epoch: 35, Batch: 10, Loss: 0.7136320562237588\n",
      "Epoch: 35, Batch: 20, Loss: 0.625999324764061\n",
      "Epoch: 35, Batch: 30, Loss: 0.5071226955712016\n",
      "Epoch: 35, Batch: 40, Loss: 0.5642260049853102\n",
      "Epoch: 35, Mean Loss: 0.7933489908687035\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 36, Batch: 0, Loss: 0.44499126375289605\n",
      "Epoch: 36, Batch: 10, Loss: 0.6059673369264736\n",
      "Epoch: 36, Batch: 20, Loss: 0.5644681645776491\n",
      "Epoch: 36, Batch: 30, Loss: 1.0854964874859374\n",
      "Epoch: 36, Batch: 40, Loss: 0.47172523297708946\n",
      "Epoch: 36, Mean Loss: 0.8565317632168636\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 37, Batch: 0, Loss: 2.663745453333517\n",
      "Epoch: 37, Batch: 10, Loss: 1.4642596826016694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-680e220cead7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mb_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-03f999fcd403>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, b)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Run optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlamda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Compute final trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-752ee6b3cd74>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mQPFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c258a96e0a3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, problem, *inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolve_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba2753206dc8>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, b, lamda)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mb_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mlamda_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda_cur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba2753206dc8>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, b, lamda)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_augmented_lagrangian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0msol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_augmented_lagrangian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mlamda\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba2753206dc8>\u001b[0m in \u001b[0;36mcompute_augmented_lagrangian\u001b[0;34m(self, b, lamda)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlamda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcost_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mlincost_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlincost_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlincost_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for batch_num, data in enumerate(train_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        if batch_num % 10 == 0:\n",
    "            print(\"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, batch_num, loss.item()))\n",
    "    \n",
    "    mean_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(mean_loss)\n",
    "    print(\"Epoch: {}, Mean Loss: {}\".format(epoch, mean_loss))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(i, traj_inp, traj_out, traj_pred):\n",
    "    traj_inp = traj_inp.numpy()\n",
    "    traj_out = traj_out.numpy()\n",
    "    traj_pred = traj_pred.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(traj_inp[::2], traj_inp[1::2], label='Inp traj')\n",
    "    ax.scatter(traj_out[:12], traj_out[12:], label='GT')\n",
    "    ax.scatter(traj_pred[:12], traj_pred[12:], label='Pred')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-20, 20])\n",
    "    ax.set_ylim([-20, 20])\n",
    "    plt.savefig('./results_fc_v2/{}.png'.format(i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 0.7403452611831383\n",
      "Batch: 1, Loss: 0.8878560839394235\n",
      "Batch: 2, Loss: 0.7715245435679279\n",
      "Batch: 3, Loss: 0.7239928365878544\n",
      "Batch: 4, Loss: 0.5795446747985719\n",
      "Batch: 5, Loss: 0.6835180643143536\n",
      "Batch: 6, Loss: 0.7633167415103669\n",
      "Batch: 7, Loss: 0.7227271313081662\n",
      "Batch: 8, Loss: 0.6911066513488902\n",
      "Batch: 9, Loss: 0.7167628329901647\n",
      "Batch: 10, Loss: 0.6758446407676194\n",
      "Batch: 11, Loss: 0.8188903861222745\n",
      "Batch: 12, Loss: 0.8621790973896707\n",
      "Batch: 13, Loss: 0.755437868409265\n",
      "Batch: 14, Loss: 0.7828438698031521\n",
      "Batch: 15, Loss: 0.9055853502940451\n",
      "Batch: 16, Loss: 0.6136821268468322\n",
      "Batch: 17, Loss: 0.6767238409357347\n",
      "Batch: 18, Loss: 0.9784208928677302\n",
      "Batch: 19, Loss: 0.9911032979739381\n",
      "Batch: 20, Loss: 1.0969428945865822\n",
      "Batch: 21, Loss: 0.6040111059618366\n",
      "Batch: 22, Loss: 0.7733043556507085\n",
      "Batch: 23, Loss: 0.7934813396594625\n",
      "Batch: 24, Loss: 0.7874994377472642\n",
      "Batch: 25, Loss: 0.8985541685982532\n",
      "Batch: 26, Loss: 0.6652912159243821\n",
      "Batch: 27, Loss: 0.8470976246177196\n",
      "Batch: 28, Loss: 0.8920994691461277\n",
      "Batch: 29, Loss: 0.6300796610557864\n",
      "Batch: 30, Loss: 0.7129315747959754\n",
      "Batch: 31, Loss: 0.6582384214123048\n",
      "Batch: 32, Loss: 0.7007406911019836\n",
      "Batch: 33, Loss: 0.8708402888045556\n",
      "Batch: 34, Loss: 0.7725999407277954\n",
      "Batch: 35, Loss: 0.993368877762085\n",
      "Batch: 36, Loss: 1.126431190070378\n",
      "Batch: 37, Loss: 0.8598208879089562\n",
      "Batch: 38, Loss: 0.9170984550166014\n",
      "Batch: 39, Loss: 0.6008021842898594\n",
      "Batch: 40, Loss: 0.7532774096604982\n",
      "Batch: 41, Loss: 0.6149093422500137\n",
      "Batch: 42, Loss: 0.6377951692304601\n",
      "Batch: 43, Loss: 0.9223106344617295\n",
      "Batch: 44, Loss: 0.7197203738149367\n",
      "Batch: 45, Loss: 0.5434884978916135\n",
      "Batch: 46, Loss: 0.6998081921257551\n",
      "Batch: 47, Loss: 0.809018507282713\n",
      "Batch: 48, Loss: 0.8491359950808751\n",
      "Batch: 49, Loss: 0.8136982379146783\n",
      "Epoch Mean Test Loss: 0.7781160467502203\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cnt = 0\n",
    "    test_loss = []\n",
    "    for batch_num, data in enumerate(test_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        print(\"Batch: {}, Loss: {}\".format(batch_num, loss.item()))\n",
    "        \n",
    "        for i in range(traj_inp.size()[0]):\n",
    "            plot_traj(cnt, traj_inp[i], traj_out[i], out[i])\n",
    "            cnt += 1\n",
    "\n",
    "mean_loss = np.mean(test_loss)\n",
    "print(\"Epoch Mean Test Loss: {}\".format(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
